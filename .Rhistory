setwd("~/R/Workspace")
install.packages("here")
install.packages("skimr")
install.packages("janitor")
install.packages("dplyr")
employee <- data.frame(id, name, job_title)
employee <- data.frame(id, name, job_title)
id <- c(1:10)
name <- c("John Mendes", "Rob Stewart", "Rachel Abrahamson", "Christy Hickman", "Johnson Harper",
"Candace Miller", "Carlson Landy", "Pansy Jordan", "Darius Berry", "Claudia Garcia")
job_title <- c("Professional", "Programmer", "Management", "Clerical", "Developer", "Programmer",
"Management", "Clerical", "Developer", "Programmer")
employee <- data.frame(id, name, job_title)
print(employee)
separate(employee, name, into = c("first_name", "last_name"), sep = " ")
package(tidyverse)
library(tidyverse)
separate(employee, name, into = c("first_name", "last_name"), sep = " ")
install.packages("SimDesign")
library(tidyverse)
ToothGrowth
library(tidyverse)
ToothGrowth %>% glimpse()
library(tidyverse)
ToothGrowth %>% skim_without_charts()
library(skimr)
library(tidyverse)
ToothGrowth %>% skim_without_charts()
library("ggplot2")
library("palmerpenguins")
library("tidyverse")
library("ggplot2")
library("palmerpenguins")
install.packages(rmarkdown)
install.packages("rmarkdown")
install.packages(twitteR)
install.packages("twitteR")
install.packages("wordcloud")
install.packages("tm")
install.packages(sentimentr)
install.packages("sentimentr")
install.packages("tidytext")
source("~/R/Workspace/Social Media Sentiment Analysis/x_smsa.Rmd")
source("~/R/Workspace/Social Media Sentiment Analysis/x_smsa.Rmd")
source("~/R/Workspace/Social Media Sentiment Analysis/x_smsa.Rmd")
source("~/R/Workspace/Social Media Sentiment Analysis/x_smsa.Rmd")
source("~/R/Workspace/Social Media Sentiment Analysis/x_smsa.Rmd")
source("~/R/Workspace/Social Media Sentiment Analysis/x_smsa.Rmd")
source("~/R/Workspace/Social Media Sentiment Analysis/x_smsa.Rmd")
source("~/R/Workspace/Social Media Sentiment Analysis/x_smsa.Rmd")
source("~/R/Workspace/Social Media Sentiment Analysis/x_smsa.Rmd")
source("~/R/Workspace/Social Media Sentiment Analysis/x_smsa.Rmd")
source("~/R/Workspace/Social Media Sentiment Analysis/x_smsa.Rmd")
source("~/R/Workspace/Social Media Sentiment Analysis/x_smsa.Rmd")
source("~/R/Workspace/Social Media Sentiment Analysis/x_smsa.Rmd")
source("~/R/Workspace/Social Media Sentiment Analysis/x_smsa.Rmd")
source("~/R/Workspace/Social Media Sentiment Analysis/x_smsa.Rmd")
source("~/R/Workspace/Social Media Sentiment Analysis/x_smsa.Rmd")
source("~/R/Workspace/Social Media Sentiment Analysis/x_smsa.Rmd")
source("~/R/Workspace/Social Media Sentiment Analysis/x_smsa.Rmd")
source("~/R/Workspace/Social Media Sentiment Analysis/x_smsa.Rmd")
source("~/R/Workspace/Social Media Sentiment Analysis/x_smsa.Rmd")
source("~/R/Workspace/Social Media Sentiment Analysis/x_smsa.Rmd")
## loading the packages
library(rtweet)
library(tidyr)
library(tidytext)
library(dplyr)
library(sentimentr)
library(wordcloud)
library(ggplot2)
## loading the credentials
credentials <- read.csv("x_credentials.csv", stringsAsFactors = FALSE)
## reading the credentials
app <- credentials$APP_name
x_api_key <- credentials$API_key
x_api_secret_key <- credentials$API_secret_key
x_bearer_token <- credentials$Bearer_token
x_access_token <- credentials$Access_token
x_access_token_secret <- credentials$Access_token_secret
## authenticating
rtweet_app(bearer_token = x_bearer_token)
## extract 200 posts from the X users
posts <- search_tweets("@google", n = 200, include_rts = FALSE)
## loading the packages
library(rtweet)
library(tidyr)
library(tidytext)
library(dplyr)
library(sentimentr)
library(wordcloud)
library(ggplot2)
## loading the credentials
credentials <- read.csv("x_credentials.csv", stringsAsFactors = FALSE)
## reading the credentials
x_app <- credentials$APP_name
bearer_tkn <- credentials$Bearer_token
api_key <- credentials$API_key
api_secret_key <- credentials$API_secret_key
access <- credentials$Access_token
access_secret <- credentials$Access_token_secret
## authenticating
token <- create_token(
app = x_app,
consumer_key = api_key,
consumer_secret = api_secret_key,
access_token = access,
access_secret = access_secret
)
View(credentials)
## loading the packages
library(rtweet)
library(tidyr)
library(tidytext)
library(dplyr)
library(sentimentr)
library(wordcloud)
library(ggplot2)
## loading the credentials
credentials <- read.csv("x_credentials.csv", stringsAsFactors = FALSE)
## reading the credentials
x_app <- credentials$APP_name
bearer_tkn <- credentials$Bearer_token
api_key <- credentials$API_key
api_secret_key <- credentials$API_secret_key
access <- credentials$Access_token
access_secret <- credentials$Access_token_secret
## authenticating
token <- create_token(
app = x_app,
consumer_key = api_key,
consumer_secret = api_secret_key,
access_token = access,
access_secret = access_secret
)
# bibliotecas utilizadas
library(twitteR)
library(tidyr)
library(tidytext)
library(dplyr)
library(sentimentr)
library(wordcloud)
library(ggplot2)
## loading the credentials
credentials <- read.csv("x_credentials.csv", stringsAsFactors = FALSE)
## reading the credentials
x_app <- credentials$APP_name
bearer_tkn <- credentials$Bearer_token
api_key <- credentials$API_key
api_secret_key <- credentials$API_secret_key
access <- credentials$Access_token
access_secret <- credentials$Access_token_secret
## authenticating
setup_twitter_oauth(
api_key,
api_secret_key,
access,
access_secret
)
# bibliotecas utilizadas
library(twitteR)
library(tidyr)
library(tidytext)
library(dplyr)
library(sentimentr)
library(wordcloud)
library(ggplot2)
## loading the credentials
credentials <- read.csv("x_credentials.csv", stringsAsFactors = FALSE)
## reading the credentials
x_app <- credentials$APP_name
bearer_tkn <- credentials$Bearer_token
api_key <- credentials$API_key
api_secret_key <- credentials$API_secret_key
access <- credentials$Access_token
access_secret <- credentials$Access_token_secret
## authenticating
setup_twitter_oauth(
api_key,
api_secret_key,
access,
access_secret
)
# bibliotecas utilizadas
library(twitteR)
library(tidyr)
library(tidytext)
library(dplyr)
library(sentimentr)
library(wordcloud)
library(ggplot2)
## loading the credentials
credentials <- read.csv("x_credentials.csv", stringsAsFactors = FALSE)
## reading the credentials
x_app <- credentials$APP_name
bearer_tkn <- credentials$Bearer_token
api_key <- credentials$API_key
api_secret_key <- credentials$API_secret_key
access_token <- credentials$Access_token
access_secret <- credentials$Access_token_secret
## authenticating
setup_twitter_oauth(
api_key,
api_secret_key,
access_token,
access_secret
)
source("~/R/Workspace/Social Media Sentiment Analysis/x_smsa.Rmd")
library(RedditExtractoR)
library(tidyverse)
library(tidytext)
library(ggplot2)
google_threads <- find_thread_urls( keywords= "google", subreddit = "technology", sort_by = "new", period = "year" )
setwd("~/R/Workspace/google_reddit_smsa")
library(RedditExtractoR)
library(tidyverse)
library(tidytext)
library(ggplot2)
google_threads <- find_thread_urls( keywords= "google", subreddit = "technology", sort_by = "new", period = "year" )
# recuperando o conteúdo do tópico pela URL
google_tc <- get_thread_content( google_threads$url )
View(google_tc)
View(google_tc[["comments"]])
View(google_tc[["comments"]])
# gerando dataframe
comments_data <- data.frame(
author = google_tc$comments$author,
comment = google_tc$comments$comment,
comment_id = google_tc$comments$comment_id,
datestamp = google_tc$comments$date
)
View(comments_data)
stop_words
# verificando a lista de palavras comuns a serem descartadas do conjunto
stop_words
ext_stop_words <- tibble(
word = c(
"https",
"amp",
"google",
"r/",
"u/"
),
lexicon = "reddit"
)
set_stop_words <- stop_words %>%
bind_rows( ext_stop_words )
suppressWarnings({
no_numbers <- comments_words %>%
filter( is.na( as.numeric( word ) ) )
})
library(RedditExtractoR)
library(tidyverse)
library(tidytext)
library(ggplot2)
google_threads <- find_thread_urls( keywords= "google", subreddit = "technology", sort_by = "new", period = "year" )
# recuperando o conteúdo do tópico pela URL
google_tc <- get_thread_content( google_threads$url )
# gerando dataframe
comments_data <- data.frame(
author = google_tc$comments$author,
comment = google_tc$comments$comment,
comment_id = google_tc$comments$comment_id,
datestamp = google_tc$comments$date
)
# extrair palavras dos comentários
comments_words <- comments_data %>%
select(
author,
comment,
comment_id,
datestamp
) %>%
unnest_tokens( word, comment )
# verificando a lista de palavras comuns a serem descartadas do conjunto
stop_words
# criando um conjunto adicional de palavras a serem descartadas
ext_stop_words <- tibble(
word = c(
"https",
"amp",
"google",
"r/",
"u/"
),
lexicon = "reddit"
)
set_stop_words <- stop_words %>%
bind_rows( ext_stop_words )
suppressWarnings({
no_numbers <- comments_words %>%
filter( is.na( as.numeric( word ) ) )
})
excl_words <- no_numbers %>%
anti_join( set_stop_words, by = "word" )
tibble(
words = nrow( comments_words ),
clean_words = nrow( excl_words )
)
tibble(
words = nrow( comments_words ),
cleaned_words = nrow( def_stop_words )
)
top_words <- excl_words %>%
group_by(word) %>%
tally %>%
arrange( desc( n ) ) %>%
head(10)
View(top_words)
top_words
dict_words <- excl_words %>%
inner_join( get_sentiments( "nrc" ), by = "word" )
dict_words
dict_words %>%
group_by( sentiment ) %>%
tally %>%
arrange( desc( n ) )
dict_words %>%
group_by( comment_id ) %>%
tally %>%
ungroup %>%
count %>%
pull
library(ggridges)
# gerando o gráfico da análise
ggplot( dict_words ) +
geom_density_ridges2(
aes(
x = datestamp,
y = sentiment,
fill = sentiment
),
alpha = 0.6,
scale = 3
) +
labs(
title = "Análise de sentimentos sobre o Google no Reddit",
x = "Data dos comentários",
y = "Sentimento"
) +
scale_fill_discrete( guide = "none" )
# gerando o gráfico da análise
ggplot( dict_words ) +
geom_density_ridges2(
aes(
x = datestamp,
y = sentiment,
fill = sentiment
),
alpha = 0.6,
scale = 5
) +
labs(
title = "Análise de sentimentos sobre o Google no Reddit",
x = "Data dos comentários",
y = "Sentimento"
) +
scale_fill_discrete( guide = "none" )
# gerando o gráfico da análise
ggplot( dict_words ) +
geom_density_ridges(
aes(
x = datestamp,
y = sentiment,
fill = sentiment
),
alpha = 0.6,
scale = 5
) +
labs(
title = "Análise de sentimentos sobre o Google no Reddit",
x = "Data dos comentários",
y = "Sentimento"
) +
scale_fill_discrete( guide = "none" )
library(wordcloud)
library(RColorBrewer)
cloud <- dict_words %>%
anti_join( excl_words, by = "comment_id" ) %>%
count %>%
{wordcloud( .$word, n, scale = c( 3.5, 0,25 ), max.words = 100 )}
cloud <- dict_words %>%
group_by( comment_id ) %>%
tally %>%
ungroup() %>%
inner_join( excl_words, by = "comment_id" ) %>%
anti_join( top_words, by = "word" ) %>%
group_by( word ) %>%
count
cloud %>%
with( wordcloud( word, n, max.words = 100, colors = rainbow( 50 ), scale = c( 3.5, 0.25 ) ) )
cloud %>%
with( wordcloud( word, n, max.words = 100, colors = rainbow( 50 ), scale = c( 3.5, 0.25 ) ) )
cloud %>%
with( wordcloud( word, n, max.words = 100, colors = rainbow( 50 ) ) )
library(ggjoy)
library(ggridges)
# gerando o gráfico da análise
ggplot( dict_words ) +
geom_joy(
aes(
x = datestamp,
y = sentiment,
fill = sentiment
),
rel_min_height = 0.01,
alpha = 0.6,
scale = 5
) +
labs(
title = "Análise de sentimentos sobre o Google no Reddit",
x = "Data dos comentários",
y = "Sentimento"
) +
scale_fill_discrete( guide = "none" )
# gerando o gráfico da análise
ggplot( dict_words ) +
geom_joy(
aes(
x = datestamp,
y = sentiment,
fill = sentiment
),
rel_min_height = 0.01,
alpha = 0.6,
scale = 5
) +
theme_joy() +
labs(
title = "Análise de sentimentos sobre o Google no Reddit",
x = "Data dos comentários",
y = "Sentimento"
) +
scale_fill_discrete( guide = "none" )
# gerando o gráfico da análise
ggplot( dict_words, aes( x = datestamp, y = sentiment) ) +
geom_density_ridges( scale = 4 ) +
theme_ridges() +
scale_y_discrete( expand = c( 0, 0 )) +
scale_x_discrete( expand = c( 0, 0 )) +
coord_cartesian( clip = "off" ) +
labs(
title = "Análise de sentimentos sobre o Google no Reddit",
x = "Data dos comentários",
y = "Sentimento"
) +
scale_fill_discrete( guide = "none" )
# gerando o gráfico da análise
ggplot( dict_words, aes( x = datestamp, y = sentiment) ) +
geom_density_ridges( scale = 4 ) +
theme_ridges() +
scale_y_discrete( expand = c( 0, 0 )) +
scale_x_discrete( expand = c( 0, 0 )) +
coord_cartesian( clip = "off" ) +
labs(
title = "Análise de sentimentos sobre o Google no Reddit",
x = "Data dos comentários",
y = "Sentimento"
)
# gerando o gráfico da análise
ggplot( dict_words, aes( x = datestamp, y = sentiment ) ) +
geom_density_ridges( scale = 10 ) +
theme_ridges() +
scale_y_discrete( expand = c( 0, 0 )) +
scale_x_discrete( expand = c( 0, 0 )) +
coord_cartesian( clip = "off" ) +
labs(
title = "Análise de sentimentos sobre o Google no Reddit",
x = "Data dos comentários",
y = "Sentimento"
)
# gerando o gráfico da análise
ggplot( dict_words, aes( x = datestamp, y = sentiment ) ) +
geom_density_ridges( scale = 100 ) +
theme_ridges() +
scale_y_discrete( expand = c( 0, 0 )) +
scale_x_discrete( expand = c( 0, 0 )) +
coord_cartesian( clip = "off" ) +
labs(
title = "Análise de sentimentos sobre o Google no Reddit",
x = "Data dos comentários",
y = "Sentimento"
)
# gerando o gráfico da análise
ggplot( dict_words, aes( x = datestamp, y = sentiment ) ) +
geom_density_ridges( scale = 3 ) +
theme_ridges() +
scale_y_discrete( expand = c( 0, 0 )) +
scale_x_discrete( expand = c( 0, 0 )) +
coord_cartesian( clip = "off" ) +
labs(
title = "Análise de sentimentos sobre o Google no Reddit",
x = "Data dos comentários",
y = "Sentimento"
)
# gerando o gráfico da análise
ggplot( dict_words, aes( x = datestamp, y = sentiment ) ) +
geom_density_ridges( scale = 1 ) +
theme_ridges() +
scale_y_discrete( expand = c( 0, 0 )) +
scale_x_discrete( expand = c( 0, 0 )) +
coord_cartesian( clip = "off" ) +
labs(
title = "Análise de sentimentos sobre o Google no Reddit",
x = "Data dos comentários",
y = "Sentimento"
)
